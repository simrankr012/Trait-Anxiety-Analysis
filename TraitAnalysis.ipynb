{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Data\n",
      "[[ 0.08777687  0.31829368]\n",
      " [ 0.36587367  1.        ]\n",
      " [ 0.46513536  0.78342904]\n",
      " [ 0.37161608  0.64479081]\n",
      " [ 0.27563577  0.73584906]\n",
      " [ 0.50287121  0.75553733]\n",
      " [ 0.41345365  0.85808039]\n",
      " [ 0.          0.10172272]\n",
      " [ 0.37079573  0.67924528]\n",
      " [ 0.42739951  0.86792453]\n",
      " [ 0.21493027  0.52830189]\n",
      " [ 0.13699754  0.3388023 ]\n",
      " [ 0.48400328  0.8876128 ]\n",
      " [ 0.34454471  0.85726005]\n",
      " [ 0.38884331  0.75553733]\n",
      " [ 0.44544709  0.7826087 ]\n",
      " [ 0.46431501  0.77522559]\n",
      " [ 0.13453651  0.40278917]\n",
      " [ 0.09187859  0.54306809]\n",
      " [ 0.05168171  0.45283019]\n",
      " [ 0.22641509  0.86792453]\n",
      " [ 0.12387203  0.57670221]\n",
      " [ 0.01558655  0.41591468]\n",
      " [ 0.16488925  0.56439705]]\n",
      "Trained Neural Network Output\n",
      "[[ 0.37875307]\n",
      " [ 0.90546033]\n",
      " [ 0.88829108]\n",
      " [ 0.53872603]\n",
      " [ 0.43347921]\n",
      " [ 0.91813693]\n",
      " [ 0.86721766]\n",
      " [ 0.45245451]\n",
      " [ 0.56913208]\n",
      " [ 0.89595727]\n",
      " [ 0.34515805]\n",
      " [ 0.37459631]\n",
      " [ 0.9569113 ]\n",
      " [ 0.71601492]\n",
      " [ 0.70039842]\n",
      " [ 0.85608466]\n",
      " [ 0.88091493]\n",
      " [ 0.35544824]\n",
      " [ 0.31470408]\n",
      " [ 0.33509646]\n",
      " [ 0.44509503]\n",
      " [ 0.31303862]\n",
      " [ 0.34502196]\n",
      " [ 0.32474946]]\n",
      "Actual Output\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "18\n",
      "75.0\n",
      "Normalized Test Data\n",
      "[[ 0.28025058  0.80019782]\n",
      " [ 0.23870755  0.85196175]\n",
      " [ 0.36102868  0.74019123]\n",
      " [ 0.59215298  1.        ]\n",
      " [ 0.30201121  0.88855918]]\n",
      "[[[ 0.49085337]]\n",
      "\n",
      " [[ 0.45467339]]\n",
      "\n",
      " [[ 0.61190389]]\n",
      "\n",
      " [[ 0.99088528]]\n",
      "\n",
      " [[ 0.64532191]]]\n",
      "[[[0]]\n",
      "\n",
      " [[0]]\n",
      "\n",
      " [[1]]\n",
      "\n",
      " [[1]]\n",
      "\n",
      " [[1]]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \n",
    "    def train(self,X,y):\n",
    "        def sigmoid (x):\n",
    "            return 1/(1 + np.exp(-x))\n",
    "        def derivatives_sigmoid(x):\n",
    "            return x * (1 - x)\n",
    "    \n",
    "        epoch=5000 #Setting training iterations\n",
    "        lr=0.1 #Setting learning rate\n",
    "        inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "        hiddenlayer_neurons = 2 #number of hidden layers neurons\n",
    "        output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "        #weight and bias initialization\n",
    "        wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "        bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "        wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "        bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "        for i in range(epoch):\n",
    "    \n",
    "\n",
    "            #Forward Propogation\n",
    "            hidden_layer_input1=np.dot(X,wh)\n",
    "    \n",
    "            hidden_layer_input=hidden_layer_input1 + bh\n",
    "    \n",
    "            hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "            output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "            output_layer_input= output_layer_input1+ bout\n",
    "            output = sigmoid(output_layer_input)\n",
    "            \n",
    "            \n",
    "            #Backpropagation\n",
    "            E = y-output\n",
    "            slope_output_layer = derivatives_sigmoid(output)\n",
    "            slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "            d_output = E * slope_output_layer\n",
    "            Error_at_hidden_layer = d_output.dot(wout.T)\n",
    "            d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "            wout += hiddenlayer_activations.T.dot(d_output) *lr\n",
    "            bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
    "            wh += X.T.dot(d_hiddenlayer) *lr\n",
    "            bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
    "        print (\"Trained Neural Network Output\")\n",
    "        print output\n",
    "        pred_value = np.where(output >= .5, 1, 0)\n",
    "        print (\"Actual Output\")\n",
    "        print (pred_value)\n",
    "        \n",
    "        nc=0\n",
    "        nf=0\n",
    "        n=len(y)\n",
    "        for i in range(len(y)):\n",
    "    \n",
    "            if(pred_value[i][0]==y[i][0]):\n",
    "                nc+=1\n",
    "            else:\n",
    "                nf+=1\n",
    "    \n",
    "\n",
    "        print (nc)\n",
    "        acc=((n-nf)/(n)*100)\n",
    "        print (acc)\n",
    "        return wh,bh,wout,bout\n",
    "        \n",
    "\n",
    "    def test(self,X,wh1,bh1,wout1,bout1):\n",
    "        def sigmoid (x):\n",
    "            return 1/(1 + np.exp(-x))\n",
    "        def derivatives_sigmoid(x):\n",
    "            return x * (1 - x)\n",
    "        epoch=5000 #Setting training iterations\n",
    "        lr=0.1 #Setting learning rate\n",
    "        inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "        hiddenlayer_neurons = 2 #number of hidden layers neurons\n",
    "        output_neurons = 1 #number of neurons at output layer\n",
    "        \n",
    "#weight and bias initialization\n",
    "        \n",
    "        for i in range(epoch):\n",
    "    \n",
    "\n",
    "#Forward Propogation\n",
    "            hidden_layer_input1=np.dot(X,wh1)\n",
    "    \n",
    "            hidden_layer_input=hidden_layer_input1 + bh1\n",
    "    \n",
    "            hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "            output_layer_input1=np.dot(hiddenlayer_activations,wout1)\n",
    "            output_layer_input= output_layer_input1+ bout1\n",
    "            output = sigmoid(output_layer_input)\n",
    "            \n",
    "        \n",
    "        print output  \n",
    "        pred_value = np.where(output >= .5, 1, 0)\n",
    "    \n",
    "        print (pred_value)\n",
    "       \n",
    "       \n",
    "#Variable initialization\n",
    "def demo():\n",
    "    \n",
    "    fdata = np.loadtxt('traindata.csv', delimiter = ',',dtype=float)\n",
    "    \n",
    "    y = fdata[:,2]\n",
    "        \n",
    "    \n",
    "    a=[]\n",
    "    list1=[]\n",
    "        \n",
    "    for j in y:\n",
    "        a.append(int(j))\n",
    "        list1.append(a)\n",
    "        a=[]\n",
    "        y=list1\n",
    "        y=np.array(y)\n",
    "     \n",
    "   \n",
    "    fdata = fdata[:,0:2] # x data\n",
    "    fdata -= fdata.min() # scaling the data so values are between 0 and 1\n",
    "    fdata /= fdata.max() # scale\n",
    "    X=fdata    \n",
    "    out=[]\n",
    "    print (\"Normalized Training Data\")\n",
    "    print X\n",
    "\n",
    "\n",
    "        # populate the tuple list with the data\n",
    "    for i in range(fdata.shape[0]):\n",
    "        fart = list((fdata[i,:].tolist(), y[i].tolist())) \n",
    "        out.append(fart)\n",
    "        fart1 = list((fdata[i,:].tolist()))\n",
    "     \n",
    "    nn=NeuralNet()\n",
    "    \n",
    "    \n",
    "    wh1,bh1,wout1,bout1=nn.train(X,y)\n",
    "    ftest = np.loadtxt('testdata.csv', delimiter = ',')\n",
    "    \n",
    "    ftest -= ftest.min() # scaling the data so values are between 0 and 1\n",
    "    ftest /= ftest.max()\n",
    "    X=ftest[:,0:2]\n",
    "    print (\"Normalized Test Data\")\n",
    "    print X\n",
    "    \n",
    "    b=[]\n",
    "    list2=[]\n",
    "        \n",
    "    for k in X:\n",
    "        b.append(k)\n",
    "        list2.append(b)\n",
    "        b=[]\n",
    "        X=list2\n",
    "        X=np.array(X)\n",
    "    \n",
    "    nn.test(X,wh1,bh1,wout1,bout1)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
